{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2575e1b7",
   "metadata": {},
   "source": [
    "# Cloud Service Scoring Evaluation\n",
    "\n",
    "This notebook provides a comprehensive evaluation framework for analyzing Cloud Elevation Index (CEI) scores generated by different AI models. It includes:\n",
    "\n",
    "- **Model Comparison**: Compare scores between Gemini 1.5 Pro and Gemini 2.0 Flash\n",
    "- **Ground Truth Validation**: Validate against known service categories\n",
    "- **Anomaly Detection**: Identify statistical outliers\n",
    "- **Visualizations**: Interactive plots and charts\n",
    "- **Comprehensive Reporting**: Generate detailed evaluation reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf38a83",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for data processing, analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import our custom utils functions\n",
    "from utils import (\n",
    "    load_old_format_scores,\n",
    "    load_new_format_scores,\n",
    "    load_multiple_score_files,\n",
    "    compare_model_scores_unified,\n",
    "    validate_dataframe_against_ground_truth,\n",
    "    export_comparison_results_to_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de05d06",
   "metadata": {},
   "source": [
    "## 2. Load Ground Truth Data\n",
    "\n",
    "Load ground truth services with expected score ranges from the configuration file.\n",
    "\n",
    "**File Structure:**\n",
    "- Configuration files: `config/` directory (prompts, ground truth, input data)\n",
    "- Score results: `scores/` directory (all scoring outputs and logs)\n",
    "- Utilities: `utils/` directory (reusable analysis functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data from JSON file in config directory\n",
    "with open('../config/ground_truth.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "    ground_truth = config['ground_truth']\n",
    "\n",
    "# Convert list ranges back to tuples for compatibility\n",
    "for service_name, service_data in ground_truth.items():\n",
    "    service_data['expected_range'] = tuple(service_data['expected_range'])\n",
    "\n",
    "print(f\"Loaded ground truth data for {len(ground_truth)} services\")\n",
    "print(\"Categories:\", set(data['category'] for data in ground_truth.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a5a74",
   "metadata": {},
   "source": [
    "## 3. Score Data Loading and Comparison\n",
    "\n",
    "The utility functions for loading different NDJSON formats and performing cross-model score comparisons are now imported from the utils package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1decc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load different format files and compare scores with run identifiers\n",
    "import os\n",
    "\n",
    "# Check available data files in both config and scores directories\n",
    "config_files = [f\"../config/{f}\" for f in os.listdir('../config') if f.endswith('.ndjson')]\n",
    "scores_files = [f\"../results/scores/{f}\" for f in os.listdir('../results/scores') if f.endswith('.ndjson')]\n",
    "available_files = config_files + scores_files\n",
    "\n",
    "print(\"Available NDJSON files:\")\n",
    "print(\"  Config files:\", [f for f in config_files])\n",
    "print(\"  Score files:\", [f for f in scores_files])\n",
    "\n",
    "# Import the new helper function\n",
    "from utils.data_loaders import load_multiple_score_files\n",
    "\n",
    "# ===== CUSTOMIZE THIS SECTION =====\n",
    "# Define your file configurations with run identifiers\n",
    "# Format: (file_path, format_type, run_identifier)\n",
    "file_configs = [\n",
    "    # Old format files\n",
    "    ('../results/scores/cei_scores.ndjson', 'old', 'baseline'),\n",
    "    \n",
    "    # New format files - you can now distinguish different runs of the same model\n",
    "    ('../results/scores/2025-10-18-19-50-26-llama-3-3-70b.ndjson', 'new', 'run1'),\n",
    "    ('../results/scores/2025-10-18-21-10-00-claude-sonnet-4-5.ndjson', 'new', 'run1'),\n",
    "    ('../results/scores/2025-10-23-15-23-13-llama-3-3-70b.ndjson', 'new', 'run2'),\n",
    "    \n",
    "    # Example of how to compare same model with different configurations:\n",
    "    # ('../results/scores/llama-with-tools.ndjson', 'new', 'with_tools'),\n",
    "    # ('../results/scores/llama-without-tools.ndjson', 'new', 'without_tools'),\n",
    "]\n",
    "\n",
    "# Load all configured files\n",
    "dataframes_to_compare = load_multiple_score_files(file_configs)\n",
    "\n",
    "print(f\"\\nLoaded {len(dataframes_to_compare)} datasets for comparison\")\n",
    "\n",
    "# Alternative: Manual loading with specific identifiers (if you prefer more control)\n",
    "\"\"\"\n",
    "dataframes_to_compare = []\n",
    "\n",
    "# Load with specific run identifiers\n",
    "if os.path.exists('../results/scores/2025-10-18-19-50-26-llama-3-3-70b.ndjson'):\n",
    "    df1 = load_new_format_scores('../results/scores/2025-10-18-19-50-26-llama-3-3-70b.ndjson', 'with_tools')\n",
    "    dataframes_to_compare.append(df1)\n",
    "\n",
    "if os.path.exists('../results/scores/2025-10-23-15-23-13-llama-3-3-70b.ndjson'):\n",
    "    df2 = load_new_format_scores('../results/scores/2025-10-23-15-23-13-llama-3-3-70b.ndjson', 'without_tools')\n",
    "    dataframes_to_compare.append(df2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Validate individual dataframes against ground truth\n",
    "print(\"\\n=== GROUND TRUTH VALIDATION ===\")\n",
    "\n",
    "# Validate each loaded dataframe against ground truth\n",
    "for i, df in enumerate(dataframes_to_compare):\n",
    "    print(f\"\\nValidating Dataset {i+1}:\")\n",
    "    \n",
    "    # Run ground truth validation\n",
    "    validation_results = validate_dataframe_against_ground_truth(df, ground_truth)\n",
    "    \n",
    "    if 'error' in validation_results:\n",
    "        print(f\"Error: {validation_results['error']}\")\n",
    "        continue\n",
    "    \n",
    "    # Display summary\n",
    "    summary = validation_results['validation_summary']\n",
    "    print(f\"  Score columns analyzed: {', '.join(validation_results['score_columns_analyzed'])}\")\n",
    "    print(f\"  Services found in data: {summary['services_found_in_data']}/{summary['total_services_in_ground_truth']}\")\n",
    "    print(f\"  Services within expected range: {summary['services_within_expected_range']}\")\n",
    "    print(f\"  Services outside expected range: {summary['services_outside_expected_range']}\")\n",
    "    print(f\"  Accuracy rate: {summary['accuracy_rate']:.2%}\")\n",
    "    \n",
    "    # Show services that don't meet ground truth (outside expected range)\n",
    "    if validation_results['services_outside_range']:\n",
    "        print(f\"\\n  === SERVICES OUTSIDE EXPECTED RANGE ===\")\n",
    "        for j, service in enumerate(validation_results['services_outside_range'][:5], 1):  # Show top 5\n",
    "            print(f\"  {j}. {service['service_name']} ({service['provider']})\")\n",
    "            print(f\"     Expected ({service['expected_category']}): {service['expected_range']}\")\n",
    "            for score_col, analysis in service['scores_analysis'].items():\n",
    "                print(f\"     {score_col}: {analysis['actual_score']:.2f} (deviation: {analysis['deviation_from_range']:.2f})\")\n",
    "            print()\n",
    "    \n",
    "    # Show services not found in data\n",
    "    if validation_results['services_not_found']:\n",
    "        print(f\"  === SERVICES NOT FOUND IN DATA ===\")\n",
    "        for service in validation_results['services_not_found'][:3]:  # Show first 3\n",
    "            print(f\"  - {service['service_name']} ({service['provider']}) - Expected: {service['expected_category']}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68f6e0",
   "metadata": {},
   "source": [
    "## 4. Usage: Loading and Comparing Multiple Score Files\n",
    "\n",
    "Examples of how to use the imported utility functions with different data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available score columns in your data:\n",
    "print(\"=== AVAILABLE SCORE COLUMNS ===\")\n",
    "all_available_columns = set()\n",
    "for i, df in enumerate(dataframes_to_compare):\n",
    "    score_cols = [col for col in df.columns if 'score' in col.lower()]\n",
    "    print(f\"Dataset {i+1}: {score_cols}\")\n",
    "    all_available_columns.update(score_cols)\n",
    "\n",
    "print(f\"\\nAll unique score columns: {sorted(list(all_available_columns))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bdce5",
   "metadata": {},
   "source": [
    "## How to Compare Different Runs of the Same Model\n",
    "\n",
    "With the updated code, you can now compare different runs of the same model by using run identifiers. Here are some common scenarios:\n",
    "\n",
    "### Scenario 1: Same model, different configurations\n",
    "```python\n",
    "file_configs = [\n",
    "    ('../results/scores/llama-with-tools.ndjson', 'new', 'with_tools'),\n",
    "    ('../results/scores/llama-without-tools.ndjson', 'new', 'without_tools'),\n",
    "]\n",
    "columns_to_compare = ['llama_3_3_70b_with_tools_score', 'llama_3_3_70b_without_tools_score']\n",
    "```\n",
    "\n",
    "### Scenario 2: Same model, different time periods\n",
    "```python\n",
    "file_configs = [\n",
    "    ('../results/scores/2025-10-18-llama.ndjson', 'new', 'oct18'),\n",
    "    ('../results/scores/2025-10-23-llama.ndjson', 'new', 'oct23'),\n",
    "]\n",
    "columns_to_compare = ['llama_3_3_70b_oct18_score', 'llama_3_3_70b_oct23_score']\n",
    "```\n",
    "\n",
    "### Scenario 3: Different versions of prompts\n",
    "```python\n",
    "file_configs = [\n",
    "    ('../results/scores/llama-prompt-v1.ndjson', 'new', 'prompt_v1'),\n",
    "    ('../results/scores/llama-prompt-v2.ndjson', 'new', 'prompt_v2'),\n",
    "]\n",
    "columns_to_compare = ['llama_3_3_70b_prompt_v1_score', 'llama_3_3_70b_prompt_v2_score']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EXAMPLE: Comparing Same Model with Different Configurations =====\n",
    "# This demonstrates how to compare Llama 3.3 70B with and without tools\n",
    "\n",
    "# If you have two different runs of the same model, configure them like this:\n",
    "example_configs = [\n",
    "    # First run - let's say this was with tools enabled\n",
    "    ('../results/scores/2025-10-18-19-50-26-llama-3-3-70b.ndjson', 'new', 'with_tools'),\n",
    "    \n",
    "    # Second run - let's say this was without tools  \n",
    "    ('../results/scores/2025-10-23-15-23-13-llama-3-3-70b.ndjson', 'new', 'without_tools'),\n",
    "]\n",
    "\n",
    "# Then you can compare specifically these two configurations:\n",
    "example_columns = ['llama_3_3_70b_with_tools_score', 'llama_3_3_70b_without_tools_score']\n",
    "\n",
    "print(\"Example configuration for comparing same model with different setups:\")\n",
    "print(\"File configs:\", example_configs)\n",
    "print(\"Columns to compare:\", example_columns)\n",
    "print(\"\\nThis allows you to see which services show different scores when using tools vs not using tools!\")\n",
    "\n",
    "# The key insight: Each run gets its own unique column name, so no data is lost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOMIZE THIS: Specify which columns you want to compare\n",
    "# Now you can compare different runs of the same model!\n",
    "\n",
    "# Example options:\n",
    "# columns_to_compare = ['claude_simple_score', 'gemini_1_5_pro_score']\n",
    "# columns_to_compare = ['cei_score', 'claude_simple_score'] \n",
    "# columns_to_compare = None  # Compare all available columns\n",
    "\n",
    "# Compare different runs of the same model:\n",
    "# columns_to_compare = ['llama_3_3_70b_with_tools_score', 'llama_3_3_70b_without_tools_score']\n",
    "# columns_to_compare = ['llama_3_3_70b_run1_score', 'llama_3_3_70b_run2_score']\n",
    "\n",
    "# Based on the current file configuration above, these columns should be available:\n",
    "columns_to_compare = ['llama_3_3_70b_run1_score', 'llama_3_3_70b_run2_score']  # Modify this list as needed\n",
    "\n",
    "print(f\"Configured to compare: {columns_to_compare}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison analysis if we have data to compare\n",
    "if len(dataframes_to_compare) >= 2:\n",
    "    print(f\"\\n=== RUNNING COMPARISON ANALYSIS ===\")\n",
    "    print(f\"Comparing {len(dataframes_to_compare)} datasets\")\n",
    "\n",
    "    if 'columns_to_compare' not in locals() or columns_to_compare is None:\n",
    "        columns_to_compare = None  # Will use all available score columns\n",
    "        print(\"Using all available score columns for comparison\")\n",
    "    else:\n",
    "        print(f\"Using specified columns for comparison: {columns_to_compare}\")\n",
    "    \n",
    "    # Run comparison with configurable thresholds\n",
    "    comparison_results = compare_model_scores_unified(\n",
    "        dataframes=dataframes_to_compare,\n",
    "        disagreement_threshold=2.0,  # Configurable threshold\n",
    "        show_similar=True,           # Set to True to see similar services too\n",
    "        similarity_threshold=1,    # Configurable similarity threshold\n",
    "        columns_to_compare=columns_to_compare  # Specify which columns to compare\n",
    "    )\n",
    "    \n",
    "    # Check for errors\n",
    "    if 'error' in comparison_results:\n",
    "        print(f\"âŒ Error: {comparison_results['error']}\")\n",
    "    else:\n",
    "        # Display results\n",
    "        print(f\"\\n=== ANALYSIS SUMMARY ===\")\n",
    "        summary = comparison_results['analysis_summary']\n",
    "        print(f\"Total services compared: {summary['total_services_compared']}\")\n",
    "        print(f\"Score columns analyzed: {', '.join(comparison_results['score_columns_analyzed'])}\")\n",
    "        print(f\"Services with high disagreement: {summary['services_with_high_disagreement']}\")\n",
    "        print(f\"Services with high similarity: {summary['services_with_high_similarity']}\")\n",
    "        \n",
    "        # Show high disagreement services\n",
    "        print(f\"\\n=== HIGH DISAGREEMENT SERVICES (deviation > {comparison_results['disagreement_threshold']}) ===\")\n",
    "        for i, service in enumerate(comparison_results['high_disagreement_services'][:10], 1):  # Show top 10\n",
    "            print(f\"{i}. {service['service_name']} ({service['provider']})\")\n",
    "            print(f\"   Median Score: {service['median_score']:.2f}\")\n",
    "            print(f\"   Max Deviation: {service['max_deviation']:.2f}\")\n",
    "            print(f\"   Scores: {', '.join([f'{k}={v:.2f}' for k, v in service['scores'].items()])}\")\n",
    "            print(f\"   Deviations: {', '.join([f'{k}={v:.2f}' for k, v in service['deviations'].items()])}\")\n",
    "            print()\n",
    "        \n",
    "        # Show high similarity services (if requested)\n",
    "        if comparison_results['high_similarity_services']:\n",
    "            print(f\"\\n=== HIGH SIMILARITY SERVICES (deviation <= {comparison_results['similarity_threshold']}) ===\")\n",
    "            for i, service in enumerate(comparison_results['high_similarity_services'][:5], 1):  # Show top 5\n",
    "                print(f\"{i}. {service['service_name']} ({service['provider']})\")\n",
    "                print(f\"   Median Score: {service['median_score']:.2f}\")\n",
    "                print(f\"   Max Deviation: {service['max_deviation']:.2f}\")\n",
    "                print(f\"   Scores: {', '.join([f'{k}={v:.2f}' for k, v in service['scores'].items()])}\")\n",
    "                print()\n",
    "        \n",
    "        # Export all results to text file\n",
    "        print(f\"\\n=== EXPORTING RESULTS ===\")\n",
    "        exported_file = export_comparison_results_to_file(comparison_results)\n",
    "        if exported_file:\n",
    "            print(f\"Complete analysis exported to: {exported_file}\")\n",
    "            print(\"This file contains ALL services that meet the criteria, not just the top ones shown above.\")\n",
    "\n",
    "else:\n",
    "    print(\"Need at least 2 datasets to run comparison analysis\")\n",
    "    print(\"Please ensure you have the appropriate NDJSON files in the current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd047383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize disagreement analysis\n",
    "def visualize_disagreement_analysis(comparison_results):\n",
    "    \"\"\"Create visualizations for the disagreement analysis.\"\"\"\n",
    "    if 'error' in comparison_results:\n",
    "        print(\"Cannot create visualizations:\", comparison_results['error'])\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # 1. Disagreement distribution\n",
    "    disagreement_services = comparison_results['high_disagreement_services']\n",
    "    if disagreement_services:\n",
    "        max_deviations = [s['max_deviation'] for s in disagreement_services]\n",
    "        axes[0].hist(max_deviations, bins=20, edgecolor='black', alpha=0.7)\n",
    "        axes[0].axvline(comparison_results['disagreement_threshold'], \n",
    "                       color='red', linestyle='--', \n",
    "                       label=f'Threshold: {comparison_results[\"disagreement_threshold\"]}')\n",
    "        axes[0].set_xlabel('Max Deviation from Median')\n",
    "        axes[0].set_ylabel('Number of Services')\n",
    "        axes[0].set_title('Distribution of High Disagreement Services')\n",
    "        axes[0].legend()\n",
    "    \n",
    "    # 2. Top disagreement services\n",
    "    if len(disagreement_services) >= 10:\n",
    "        top_services = disagreement_services[:10]\n",
    "        service_names = [s['service_name'][:15] + '...' if len(s['service_name']) > 15 \n",
    "                        else s['service_name'] for s in top_services]\n",
    "        deviations = [s['max_deviation'] for s in top_services]\n",
    "        \n",
    "        axes[1].barh(range(len(service_names)), deviations)\n",
    "        axes[1].set_yticks(range(len(service_names)))\n",
    "        axes[1].set_yticklabels(service_names)\n",
    "        axes[1].set_xlabel('Max Deviation from Median')\n",
    "        axes[1].set_title('Top 10 Services with Highest Disagreement')\n",
    "        axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualization if we have comparison results\n",
    "if 'comparison_results' in locals():\n",
    "    visualize_disagreement_analysis(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764294d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
